{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5458a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/2000 | Average Score: -1231.74 | Epsilon: 0.6058\n",
      "Episode 200/2000 | Average Score: -389.69 | Epsilon: 0.3670\n",
      "Episode 300/2000 | Average Score: -521.50 | Epsilon: 0.2223\n",
      "Episode 400/2000 | Average Score: -493.58 | Epsilon: 0.1347\n",
      "Episode 500/2000 | Average Score: -264.79 | Epsilon: 0.0816\n",
      "模型已保存到 models/dqn_agent_firm_1_episode_500.pth\n",
      "Episode 600/2000 | Average Score: 90.86 | Epsilon: 0.0494\n",
      "Episode 700/2000 | Average Score: 237.62 | Epsilon: 0.0299\n",
      "Episode 800/2000 | Average Score: 338.11 | Epsilon: 0.0181\n",
      "Episode 900/2000 | Average Score: 377.18 | Epsilon: 0.0110\n",
      "Episode 1000/2000 | Average Score: 399.65 | Epsilon: 0.0100\n",
      "模型已保存到 models/dqn_agent_firm_1_episode_1000.pth\n",
      "Episode 1100/2000 | Average Score: 415.57 | Epsilon: 0.0100\n",
      "Episode 1200/2000 | Average Score: 444.55 | Epsilon: 0.0100\n",
      "Episode 1300/2000 | Average Score: 430.06 | Epsilon: 0.0100\n",
      "Episode 1400/2000 | Average Score: 426.19 | Epsilon: 0.0100\n",
      "Episode 1500/2000 | Average Score: 418.94 | Epsilon: 0.0100\n",
      "模型已保存到 models/dqn_agent_firm_1_episode_1500.pth\n",
      "Episode 1600/2000 | Average Score: 384.78 | Epsilon: 0.0100\n",
      "Episode 1700/2000 | Average Score: 400.64 | Epsilon: 0.0100\n",
      "Episode 1800/2000 | Average Score: 398.08 | Epsilon: 0.0100\n",
      "Episode 1900/2000 | Average Score: 417.55 | Epsilon: 0.0100\n",
      "Episode 2000/2000 | Average Score: 397.73 | Epsilon: 0.0100\n",
      "模型已保存到 models/dqn_agent_firm_1_episode_2000.pth\n",
      "模型已保存到 models/dqn_agent_firm_1_final.pth\n",
      "Test Episode 1/10 | Score: 359.50\n",
      "Test Episode 2/10 | Score: 421.00\n",
      "Test Episode 3/10 | Score: 579.50\n",
      "Test Episode 4/10 | Score: 387.00\n",
      "Test Episode 5/10 | Score: 425.00\n",
      "Test Episode 6/10 | Score: 441.50\n",
      "Test Episode 7/10 | Score: 318.50\n",
      "Test Episode 8/10 | Score: 247.00\n",
      "Test Episode 9/10 | Score: 549.00\n",
      "Test Episode 10/10 | Score: 385.00\n"
     ]
    }
   ],
   "source": [
    "# 复制环境类\n",
    "class Env:\n",
    "    def __init__(self, num_firms, p, h, c, initial_inventory, poisson_lambda=10, max_steps=100):\n",
    "        \"\"\"\n",
    "        初始化供应链管理仿真环境。\n",
    "        \n",
    "        :param num_firms: 企业数量\n",
    "        :param p: 各企业的价格列表\n",
    "        :param h: 库存持有成本\n",
    "        :param c: 损失销售成本\n",
    "        :param initial_inventory: 每个企业的初始库存\n",
    "        :param poisson_lambda: 最下游企业需求的泊松分布均值\n",
    "        :param max_steps: 每个episode的最大步数\n",
    "        \"\"\"\n",
    "        self.num_firms = num_firms\n",
    "        self.p = p  # 企业的价格列表\n",
    "        self.h = h  # 库存持有成本\n",
    "        self.c = c  # 损失销售成本\n",
    "        self.poisson_lambda = poisson_lambda  # 泊松分布的均值\n",
    "        self.max_steps = max_steps  # 每个episode的最大步数\n",
    "        self.initial_inventory = initial_inventory  # 初始库存\n",
    "        \n",
    "        # 初始化库存\n",
    "        self.inventory = np.full((num_firms, 1), initial_inventory)\n",
    "        # 初始化订单量\n",
    "        self.orders = np.zeros((num_firms, 1))\n",
    "        # 初始化已满足的需求量\n",
    "        self.satisfied_demand = np.zeros((num_firms, 1))\n",
    "        # 记录当前步数\n",
    "        self.current_step = 0\n",
    "        # 标记episode是否结束\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境状态。\n",
    "        \"\"\"\n",
    "        self.inventory = np.full((self.num_firms, 1), self.initial_inventory)\n",
    "        self.orders = np.zeros((self.num_firms, 1))\n",
    "        self.satisfied_demand = np.zeros((self.num_firms, 1))\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        获取每个企业的观察信息，包括订单量、满足的需求量和库存。\n",
    "        每个企业的状态是独立的，包括自己观察的订单、需求和库存。\n",
    "        \"\"\"\n",
    "        return np.concatenate((self.orders, self.satisfied_demand, self.inventory), axis=1)\n",
    "\n",
    "    def _generate_demand(self):\n",
    "        \"\"\"\n",
    "        根据规则生成每个企业的需求。\n",
    "        最下游企业的需求遵循泊松分布，其他企业的需求等于下游企业的订单量。\n",
    "        \"\"\"\n",
    "        demand = np.zeros((self.num_firms, 1))\n",
    "        for i in range(self.num_firms):\n",
    "            if i == 0:\n",
    "                # 最下游企业的需求遵循泊松分布，均值为 poisson_lambda\n",
    "                demand[i] = np.random.poisson(self.poisson_lambda)\n",
    "            else:\n",
    "                # 上游企业的需求等于下游企业的订单量\n",
    "                demand[i] = self.orders[i - 1]  # d_{i+1,t} = q_{it}\n",
    "        return demand\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        执行一个时间步的仿真，根据给定的行动 (每个企业的订单量) 更新环境状态。\n",
    "        \n",
    "        :param actions: 每个企业的订单量 (shape: (num_firms, 1))，即每个智能体的行动\n",
    "        :return: next_state, reward, done\n",
    "        \"\"\"\n",
    "        self.orders = actions  # 更新订单量\n",
    "        \n",
    "        # 生成各企业的需求\n",
    "        self.demand = self._generate_demand()\n",
    "\n",
    "        # 计算每个企业收到的订单量和满足的需求\n",
    "        for i in range(self.num_firms):\n",
    "            if i == 0:\n",
    "                # 第一企业从外部需求直接得到满足\n",
    "                self.satisfied_demand[i] = min(self.demand[i], self.inventory[i])\n",
    "            else:\n",
    "                # 后续企业的需求由上游企业订单决定\n",
    "                self.satisfied_demand[i] = min(self.demand[i], self.inventory[i])\n",
    "        \n",
    "        # 更新库存\n",
    "        for i in range(self.num_firms):\n",
    "            self.inventory[i] = self.inventory[i] + self.orders[i] - self.satisfied_demand[i]\n",
    "        \n",
    "        # 计算每个企业的奖励: p_i * d_{it} - p_{i+1} * q_{it} - h * I_{it}\n",
    "        rewards = np.zeros((self.num_firms, 1))\n",
    "        loss_sales = np.zeros((self.num_firms, 1))  # 损失销售费用\n",
    "        \n",
    "        for i in range(self.num_firms):\n",
    "            rewards[i] += self.p[i] * self.satisfied_demand[i] - (self.p[i+1] if i+1 < self.num_firms else 0) * self.orders[i] - self.h * self.inventory[i]\n",
    "            \n",
    "            # 损失销售计算\n",
    "            if self.satisfied_demand[i] < self.demand[i]:\n",
    "                loss_sales[i] = (self.demand[i] - self.satisfied_demand[i]) * self.c\n",
    "        \n",
    "        rewards -= loss_sales  # 总奖励扣除损失销售成本\n",
    "        \n",
    "        # 增加步数\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # 判断是否结束（比如达到最大步数）\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "        \n",
    "        return self._get_observation(), rewards, self.done\n",
    "\n",
    "# 定义Q网络模型\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"\n",
    "        初始化Q网络\n",
    "        \n",
    "        :param state_size: 状态空间维度\n",
    "        :param action_size: 动作空间维度\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        :param state: 输入状态\n",
    "        :return: 各动作的Q值\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        初始化经验回放缓冲区\n",
    "        \n",
    "        :param capacity: 缓冲区容量\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        添加经验到缓冲区\n",
    "        \n",
    "        :param state: 当前状态\n",
    "        :param action: 执行的动作\n",
    "        :param reward: 获得的奖励\n",
    "        :param next_state: 下一个状态\n",
    "        :param done: 是否结束\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        从缓冲区采样一批经验\n",
    "        \n",
    "        :param batch_size: 批大小\n",
    "        :return: 一批经验 (state, action, reward, next_state, done)\n",
    "        \"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        获取缓冲区当前大小\n",
    "        \n",
    "        :return: 缓冲区大小\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 定义DQN智能体\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, firm_id, max_order=20, buffer_size=10000, batch_size=64, gamma=0.99, \n",
    "                 learning_rate=1e-3, tau=1e-3, update_every=4):\n",
    "        \"\"\"\n",
    "        初始化DQN智能体\n",
    "        \n",
    "        :param state_size: 状态空间维度\n",
    "        :param action_size: 动作空间维度\n",
    "        :param firm_id: 企业ID，用于标识训练哪个企业\n",
    "        :param max_order: 最大订单量，用于离散化动作空间\n",
    "        :param buffer_size: 回放缓冲区大小\n",
    "        :param batch_size: 批大小\n",
    "        :param gamma: 折扣因子\n",
    "        :param learning_rate: 学习率\n",
    "        :param tau: 软更新参数\n",
    "        :param update_every: 更新目标网络的频率\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.firm_id = firm_id\n",
    "        self.max_order = max_order\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.learning_step = 0\n",
    "        \n",
    "        # 创建Q网络和目标网络\n",
    "        self.q_network = QNetwork(state_size, action_size)\n",
    "        self.target_network = QNetwork(state_size, action_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # 设置优化器\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # 创建经验回放缓冲区\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # 跟踪训练进度\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        添加经验到回放缓冲区并按需学习\n",
    "        \n",
    "        :param state: 当前状态\n",
    "        :param action: 执行的动作\n",
    "        :param reward: 获得的奖励\n",
    "        :param next_state: 下一个状态\n",
    "        :param done: 是否结束\n",
    "        \"\"\"\n",
    "        # 添加经验到回放缓冲区\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # 每隔一定步数学习\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            self.learn(experiences)\n",
    "    \n",
    "    def act(self, state, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        根据当前状态选择动作\n",
    "        \n",
    "        :param state: 当前状态\n",
    "        :param epsilon: epsilon-贪婪策略参数\n",
    "        :return: 选择的动作\n",
    "        \"\"\"\n",
    "        # 从3维numpy数组转换为1维向量\n",
    "        state = torch.from_numpy(state.flatten()).float().unsqueeze(0)\n",
    "        \n",
    "        # 切换到评估模式\n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.q_network(state)\n",
    "        # 切换回训练模式\n",
    "        self.q_network.train()\n",
    "        \n",
    "        # epsilon-贪婪策略\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy()) + 1  # +1 因为我们的动作从1开始\n",
    "        else:\n",
    "            return random.randint(1, self.max_order)\n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        \"\"\"\n",
    "        从经验批次中学习\n",
    "        \n",
    "        :param experiences: (state, action, reward, next_state, done) 元组\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        # 转换为torch张量\n",
    "        states = torch.from_numpy(np.vstack([s.flatten() for s in states])).float()\n",
    "        actions = torch.from_numpy(np.vstack([a-1 for a in actions])).long()  # -1 因为我们的动作从1开始，但索引从0开始\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float()\n",
    "        next_states = torch.from_numpy(np.vstack([ns.flatten() for ns in next_states])).float()\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float()\n",
    "        \n",
    "        # 从目标网络获取下一个状态的最大预测Q值\n",
    "        Q_targets_next = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # 计算目标Q值\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # 获取当前Q值估计\n",
    "        Q_expected = self.q_network(states).gather(1, actions)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = nn.MSELoss()(Q_expected, Q_targets)\n",
    "        \n",
    "        # 最小化损失\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 更新目标网络\n",
    "        self.learning_step += 1\n",
    "        if self.learning_step % self.update_every == 0:\n",
    "            self.soft_update()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def soft_update(self):\n",
    "        \"\"\"\n",
    "        软更新目标网络参数：θ_target = τ*θ_local + (1-τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        保存模型参数\n",
    "        \n",
    "        :param filename: 文件名\n",
    "        \"\"\"\n",
    "        # 确保目录存在\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        # 保存模型状态字典\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filename)\n",
    "        print(f\"模型已保存到 {filename}\")\n",
    "    \n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        加载模型参数\n",
    "        \n",
    "        :param filename: 文件名\n",
    "        \"\"\"\n",
    "        if os.path.isfile(filename):\n",
    "            checkpoint = torch.load(filename)\n",
    "            self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "            self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f\"从 {filename} 加载了模型\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def train_dqn(env, agent, num_episodes=1000, max_t=100, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    训练DQN智能体\n",
    "    \n",
    "    :param env: 环境\n",
    "    :param agent: DQN智能体\n",
    "    :param num_episodes: 训练的episodes数量\n",
    "    :param max_t: 每个episode的最大步数\n",
    "    :param eps_start: 起始epsilon值\n",
    "    :param eps_end: 最小epsilon值\n",
    "    :param eps_decay: epsilon衰减率\n",
    "    :return: 所有episode的奖励\n",
    "    \"\"\"\n",
    "    scores = []  # 每个episode的总奖励\n",
    "    eps = eps_start  # 初始epsilon值\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            # 对特定企业采取动作，其他企业随机决策\n",
    "            actions = np.zeros((env.num_firms, 1))\n",
    "            for firm_id in range(env.num_firms):\n",
    "                if firm_id == agent.firm_id:\n",
    "                    # 使用智能体策略\n",
    "                    firm_state = state[firm_id].reshape(1, -1)\n",
    "                    action = agent.act(firm_state, eps)\n",
    "                    actions[firm_id] = action\n",
    "                else:\n",
    "                    # 对其他企业采取随机策略\n",
    "                    actions[firm_id] = np.random.randint(1, 21)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, rewards, done = env.step(actions)\n",
    "            \n",
    "            # 该企业的奖励\n",
    "            reward = rewards[agent.firm_id][0]\n",
    "            \n",
    "            # 保存经验并学习\n",
    "            agent.step(state[agent.firm_id].reshape(1, -1), actions[agent.firm_id], reward, next_state[agent.firm_id].reshape(1, -1), done)\n",
    "            \n",
    "            # 更新状态和奖励\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 更新epsilon\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        # 记录分数\n",
    "        scores.append(score)\n",
    "        \n",
    "        # 输出进度\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}/{num_episodes} | Average Score: {np.mean(scores[-100:]):.2f} | Epsilon: {eps:.4f}')\n",
    "        \n",
    "        # 每隔一定episode保存模型\n",
    "        if i_episode % 500 == 0:\n",
    "            agent.save(f'models/dqn_agent_firm_{agent.firm_id}_episode_{i_episode}.pth')\n",
    "    \n",
    "    # 训练结束后保存最终模型\n",
    "    agent.save(f'models/dqn_agent_firm_{agent.firm_id}_final.pth')\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def test_agent(env, agent, num_episodes=10):\n",
    "    \"\"\"\n",
    "    测试训练好的DQN智能体\n",
    "    \n",
    "    :param env: 环境\n",
    "    :param agent: 训练好的DQN智能体\n",
    "    :param num_episodes: 测试的episodes数量\n",
    "    :return: 所有episode的奖励和详细信息\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    inventory_history = []\n",
    "    orders_history = []\n",
    "    demand_history = []\n",
    "    satisfied_demand_history = []\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        episode_inventory = []\n",
    "        episode_orders = []\n",
    "        episode_demand = []\n",
    "        episode_satisfied_demand = []\n",
    "        \n",
    "        for t in range(env.max_steps):\n",
    "            # 对特定企业采取动作，其他企业随机决策\n",
    "            actions = np.zeros((env.num_firms, 1))\n",
    "            for firm_id in range(env.num_firms):\n",
    "                if firm_id == agent.firm_id:\n",
    "                    # 使用智能体策略，不使用探索\n",
    "                    firm_state = state[firm_id].reshape(1, -1)\n",
    "                    action = agent.act(firm_state, epsilon=0.0)\n",
    "                    actions[firm_id] = action\n",
    "                else:\n",
    "                    # 对其他企业采取随机策略\n",
    "                    actions[firm_id] = np.random.randint(1, 21)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, rewards, done = env.step(actions)\n",
    "            \n",
    "            # 记录关键指标\n",
    "            episode_inventory.append(env.inventory[agent.firm_id][0])\n",
    "            episode_orders.append(actions[agent.firm_id][0])\n",
    "            episode_demand.append(env.demand[agent.firm_id][0])\n",
    "            episode_satisfied_demand.append(env.satisfied_demand[agent.firm_id][0])\n",
    "            \n",
    "            # 该企业的奖励\n",
    "            reward = rewards[agent.firm_id][0]\n",
    "            score += reward\n",
    "            \n",
    "            # 更新状态\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 记录分数和历史数据\n",
    "        scores.append(score)\n",
    "        inventory_history.append(episode_inventory)\n",
    "        orders_history.append(episode_orders)\n",
    "        demand_history.append(episode_demand)\n",
    "        satisfied_demand_history.append(episode_satisfied_demand)\n",
    "        \n",
    "        print(f'Test Episode {i_episode}/{num_episodes} | Score: {score:.2f}')\n",
    "    \n",
    "    return scores, inventory_history, orders_history, demand_history, satisfied_demand_history\n",
    "\n",
    "def plot_training_results(scores, window_size=100):\n",
    "    \"\"\"\n",
    "    绘制训练结果\n",
    "    \n",
    "    :param scores: 每个episode的奖励\n",
    "    :param window_size: 移动平均窗口大小\n",
    "    \"\"\"\n",
    "    # 计算移动平均\n",
    "    def moving_average(data, window_size):\n",
    "        return [np.mean(data[max(0, i-window_size):i+1]) for i in range(len(data))]\n",
    "    \n",
    "    avg_scores = moving_average(scores, window_size)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.arange(len(scores)), scores, alpha=0.3, label='原始奖励')\n",
    "    plt.plot(np.arange(len(avg_scores)), avg_scores, label=f'{window_size}个episode的移动平均')\n",
    "    plt.title('DQN训练过程中的奖励')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('奖励')\n",
    "    plt.legend()\n",
    "    plt.savefig('figures/training_rewards.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_test_results(scores, inventory_history, orders_history, demand_history, satisfied_demand_history):\n",
    "    \"\"\"\n",
    "    绘制测试结果\n",
    "    \n",
    "    :param scores: 每个episode的奖励\n",
    "    :param inventory_history: 每个episode的库存历史\n",
    "    :param orders_history: 每个episode的订单历史\n",
    "    :param demand_history: 每个episode的需求历史\n",
    "    :param satisfied_demand_history: 每个episode的满足需求历史\n",
    "    \"\"\"\n",
    "    # 计算平均值，用于绘图\n",
    "    avg_inventory = np.mean(inventory_history, axis=0)\n",
    "    avg_orders = np.mean(orders_history, axis=0)\n",
    "    avg_demand = np.mean(demand_history, axis=0)\n",
    "    avg_satisfied_demand = np.mean(satisfied_demand_history, axis=0)\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 库存图表\n",
    "    axs[0, 0].plot(avg_inventory)\n",
    "    axs[0, 0].set_title('平均库存')\n",
    "    axs[0, 0].set_xlabel('时间步')\n",
    "    axs[0, 0].set_ylabel('库存量')\n",
    "    \n",
    "    # 订单图表\n",
    "    axs[0, 1].plot(avg_orders)\n",
    "    axs[0, 1].set_title('平均订单量')\n",
    "    axs[0, 1].set_xlabel('时间步')\n",
    "    axs[0, 1].set_ylabel('订单量')\n",
    "    \n",
    "    # 需求和满足需求图表\n",
    "    axs[1, 0].plot(avg_demand, label='需求')\n",
    "    axs[1, 0].plot(avg_satisfied_demand, label='满足的需求')\n",
    "    axs[1, 0].set_title('平均需求 vs 满足的需求')\n",
    "    axs[1, 0].set_xlabel('时间步')\n",
    "    axs[1, 0].set_ylabel('数量')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # 奖励柱状图\n",
    "    axs[1, 1].bar(range(len(scores)), scores)\n",
    "    axs[1, 1].set_title('测试episode奖励')\n",
    "    axs[1, 1].set_xlabel('Episode')\n",
    "    axs[1, 1].set_ylabel('总奖励')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/test_results.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建保存模型和图表的目录\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "    \n",
    "    # 初始化环境参数\n",
    "    num_firms = 3  # 假设有3个企业\n",
    "    p = [10, 9, 8]  # 价格列表\n",
    "    h = 0.5  # 库存持有成本\n",
    "    c = 2  # 损失销售成本\n",
    "    initial_inventory = 100  # 初始库存\n",
    "    poisson_lambda = 10  # 泊松分布的均值\n",
    "    max_steps = 100  # 每个episode的最大步数\n",
    "    \n",
    "    # 创建仿真环境\n",
    "    env = Env(num_firms, p, h, c, initial_inventory, poisson_lambda, max_steps)\n",
    "    \n",
    "    # 为第二个企业创建DQN智能体\n",
    "    firm_id = 1  # 选择第二个企业进行训练\n",
    "    state_size = 3  # 每个企业的状态维度：订单、满足的需求和库存\n",
    "    action_size = 20  # 假设最大订单量为20\n",
    "    \n",
    "    agent = DQNAgent(state_size=state_size, action_size=action_size, firm_id=firm_id, max_order=action_size)\n",
    "    \n",
    "    # 训练DQN智能体\n",
    "    scores = train_dqn(env, agent, num_episodes=2000, max_t=max_steps, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "    \n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 指定中文字体\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 绘图显示负号\n",
    "\n",
    "    # 绘制训练结果\n",
    "    plot_training_results(scores)\n",
    "    \n",
    "    # 测试训练好的智能体\n",
    "    test_scores, inventory_history, orders_history, demand_history, satisfied_demand_history = test_agent(env, agent, num_episodes=10)\n",
    "    \n",
    "    # 绘制测试结果\n",
    "    plot_test_results(test_scores, inventory_history, orders_history, demand_history, satisfied_demand_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
